{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tobig's 15기 2주차 Optimization 과제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent 구현하기\n",
    "\n",
    "### 1)\"...\"표시되어 있는 빈 칸을 채워주세요\n",
    "### 2)강의내용과 코드에 대해 공부한 내용을 마크마운 또는 주석으로 설명해주세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.2</td>\n",
       "      <td>63000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>76000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  bias  experience  salary\n",
       "0      1     1         0.7   48000\n",
       "1      0     1         1.9   48000\n",
       "2      1     1         2.5   60000\n",
       "3      0     1         4.2   63000\n",
       "4      0     1         6.0   76000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('assignment_2.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test 데이터 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:, 0], test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 3), (50, 3), (150,), (50,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "experience와 salary의 단위, 평균, 분산이 크게 차이나므로 scaler를 사용해 단위를 맞춰줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.187893</td>\n",
       "      <td>-1.143335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.185555</td>\n",
       "      <td>0.043974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.310938</td>\n",
       "      <td>-0.351795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.629277</td>\n",
       "      <td>-1.341220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.308600</td>\n",
       "      <td>0.043974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bias  experience    salary\n",
       "0     1    0.187893 -1.143335\n",
       "1     1    1.185555  0.043974\n",
       "2     1   -0.310938 -0.351795\n",
       "3     1   -1.629277 -1.341220\n",
       "4     1   -1.308600  0.043974"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "bias_train = X_train[\"bias\"]\n",
    "bias_train = bias_train.reset_index()[\"bias\"]\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\n",
    "X_train[\"bias\"] = bias_train\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이때 scaler는 X_train에 fit 해주시고, fit한 scaler를 X_test에 적용시켜줍니다.  \n",
    "똑같이 X_test에다 fit하면 안돼요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.344231</td>\n",
       "      <td>-0.615642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.508570</td>\n",
       "      <td>0.307821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.310938</td>\n",
       "      <td>0.571667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.363709</td>\n",
       "      <td>1.956862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.987923</td>\n",
       "      <td>-0.747565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bias  experience    salary\n",
       "0     1   -1.344231 -0.615642\n",
       "1     1    0.508570  0.307821\n",
       "2     1   -0.310938  0.571667\n",
       "3     1    1.363709  1.956862\n",
       "4     1   -0.987923 -0.747565"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_test = X_test[\"bias\"]\n",
    "bias_test = bias_test.reset_index()[\"bias\"]\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)\n",
    "X_test[\"bias\"] = bias_test\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter 개수\n",
    "N = len(X_train.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33859021, 0.0079147 , 0.66255967])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 초기 parameter들을 임의로 설정해줍니다.\n",
    "parameters = np.array([random.random() for i in range(N)])\n",
    "random_parameters = parameters.copy()\n",
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * LaTeX   \n",
    "\n",
    "Jupyter Notebook은 LaTeX 문법으로 수식 입력을 지원하고 있습니다.  \n",
    "LaTeX문법으로 아래의 수식을 완성해주세요  \n",
    "http://triki.net/apps/3466  \n",
    "https://jjycjnmath.tistory.com/117"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot product\n",
    "## $z = X_i \\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(X, parameters):\n",
    "    z = 0\n",
    "    for i in range(len(parameters)):\n",
    "        z += X[i] * parameters[i]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Function\n",
    "\n",
    "## $p(z) = \\frac{1}{1 + e^{-z}} = \\frac{1}{1 + e^{-X_i\\theta}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(X, parameters):\n",
    "    z = dot_product(X, parameters)\n",
    "    p = 1 / (1 + np.exp(-z))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.593175690529389"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic(X_train.iloc[1], parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object function\n",
    "\n",
    "Object Function : 목적함수는 Gradient Descent를 통해 최적화 하고자 하는 함수입니다.  \n",
    "<br>\n",
    "선형 회귀의 목적함수\n",
    "## $l(\\theta) = \\frac{1}{2}\\Sigma(y_i - \\theta^{T}X_i)^2$  \n",
    "참고) $\\hat{y_i} = \\theta^{T}X_i$\n",
    "  \n",
    "로지스틱 회귀의 목적함수를 작성해주세요  \n",
    "(선형 회귀의 목적함수처럼 강의에 나온대로 작성해주세요. 평균을 고려하는 것은 뒤에 코드에서 수행합니다)\n",
    "## $l(p) = -\\Sigma{(y_ilog(p) + (1-y_i)log(1-p))}$\n",
    "※ $p = \\frac{1}{1 + e^{-X_i\\theta}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minus_log_cross_entropy_i(X, y, parameters):\n",
    "    p = logistic(X, parameters)\n",
    "    loss = -(y * np.log(p) + (1 - y) * (np.log(1 - p)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_i(X, y, parameters):\n",
    "    y_hat = dot_product(X, parameters)\n",
    "    loss = 1 / 2 * (y - y_hat) ** 2\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_loss(X_set, y_set, parameters, loss_function, n): #n:현재 배치의 데이터 수\n",
    "    loss = 0\n",
    "    for i in range(X_set.shape[0]):\n",
    "        X = X_set.iloc[i,:]\n",
    "        y = y_set.iloc[i]\n",
    "        loss += loss_function(X, y, parameters)\n",
    "    loss /= n\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.995407214181738"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_loss(X_test, y_test, parameters, minus_log_cross_entropy_i, len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "위의 선형회귀의 목적함수 $l(\\theta)$와 로지스틱회귀의 목적함수 $l(p)$의 gradient를 작성해주세요  \n",
    "(위의 목적함수를 참고해서 작성해주세요 = 평균을 고려하는 것은 뒤에 코드에서 수행합니다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ${\\partial\\over{\\partial \\theta_j}}l(\\theta)=-\\Sigma{(y_i-\\theta^TX_i)X_{ij}}$ \n",
    "## ${\\partial\\over{\\partial \\theta_j}}l(p)=-\\Sigma{(y_i - p_i)X_{ij}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_ij(X, y, parameters, j, model):\n",
    "    if model == 'linear':\n",
    "        y_hat = dot_product(X, parameters)\n",
    "        gradient = -(y - y_hat) * X[j]\n",
    "    else:\n",
    "        p = logistic(X, parameters)\n",
    "        gradient = -(y - p) * X[j]\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.11327563534145814"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gradient_ij(X_train.iloc[0,:], y_train.iloc[0], parameters, 1, 'logistic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient\n",
    "하나의 배치 (X_set, y_set)에 대해 기울기를 구하는 코드를 작성해주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient(X_set, y_set, parameters, model):\n",
    "    gradients = [0 for _ in range(len(parameters))]\n",
    "    \n",
    "    for i in range(X_set.shape[0]):\n",
    "        X = X_set.iloc[i,:]\n",
    "        y = y_set.iloc[i]\n",
    "        for j in range(len(parameters)):\n",
    "            gradients[j] += get_gradient_ij(X, y, parameters, j, model)\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[44.24935516909498, -1.4480535879497967, 36.20363658950191]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients1 = batch_gradient(X_train, y_train, parameters, 'logistic')\n",
    "gradients1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mini-batch\n",
    "인덱스로 미니 배치 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_idx(X_train, batch_size):\n",
    "    N = len(X_train)\n",
    "    nb = (N // batch_size)+1 #number of batch\n",
    "    idx = np.array([i for i in range(N)])\n",
    "    idx_list = [idx[i*batch_size:(i+1)*batch_size] for i in range(nb) if len(idx[i*batch_size:(i+1)*batch_size]) != 0]\n",
    "    return idx_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_idx 함수에 대한 설명을 batch_size와 함께 간략하게 작성해주세요  \n",
    "### 설명: \n",
    "1. dataset과  batch_size를 parameter로 받습니다.\n",
    "2. 전체 sample수를 batch_size로 나누어 1을 더해 전체 배치 수를 구합니다.\n",
    "3. for문을 사용하여 batch_size만큼 나누어진 원소가 numpy array 배열을 return합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Parameters\n",
    "기울기를 갱신하는 코드를 작성해주세요  \n",
    "(loss와 마찬가지로 기울기를 갱신할 때 배치 사이즈를 고려해 평균으로 갱신해주세요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(parameters, gradients, learning_rate, n): #n:현재 배치의 데이터 수\n",
    "    for i in range(len(parameters)):\n",
    "        gradients[i] *= learning_rate / n\n",
    "    parameters -= gradients\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33564025, 0.00801124, 0.66014609])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step(parameters, gradients1, 0.01, len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "위에서 작성한 함수들을 조합해서 경사하강법 함수를 완성해주세요\n",
    "\n",
    "- learning_rate: 학습률  \n",
    "- tolerance: Step이 너무 작아서 더 이상의 학습이 무의미할 때 학습을 멈추는 조건  \n",
    "- batch: 기울기를 1번 갱신할 때 사용하는 데이터셋  \n",
    "- epoch: 훈련집합 전체에 대해 gradient descent를 적용함. 즉, 훈련집합의 모든 sample을 훈련시킴 \n",
    "- num_epoch: 전체 훈련시킬 epoch 수\n",
    "<br>\n",
    "\n",
    "BGD: epoch당 한번의 학습으로 전체 훈련집합에 대한 기울기 갱신이 이루어지지만 훈련집합이 매우 클 경우 메모리문제가 발생할 수 있음. 전체 데이터에 대해 loss를 계산하기 때문에 안정적으로 최적 parameter를 계산할 수 있지만, parameter 갱신 수가 극도로 낮으므로 매우 느림. convex문제가 아닐 경우 지역최적화문제가 있음. \n",
    "\n",
    "SGD: 하나의 sample에 대해 기울기 갱신이 이루어지므로 epoch당 sample의 수만큼 학습함. 1개의 sample을 통해 기울기 갱신이 이루어지므로 loss가 튀는 현상때문에 parameter의 안정적인 수렴이 어려움. 쉬운 문제의 경우 가장 빠르게 최적점에 도달할 수 있음.\n",
    "\n",
    "MGD: 한번에 k(0 < k < n_samples)개의 sample에 대해 학습함. sgd에 비해 안정적인 수렴 + 병렬처리 가능. bgd에 비해 빠른 계산 + 지역최적화문제 해결\n",
    "<br>\n",
    "<br>\n",
    "batch_size에 따른 경사하강법의 종류를 적어주세요  \n",
    "batch_size=1 -> Stochastic Gradient Descent  \n",
    "batch_size=k -> mini-batch Gradient Descent  \n",
    "batch_size=whole -> Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X_train, y_train, learning_rate = 0.1, num_epoch = 1000, tolerance = 0.00001, model = 'logistic', batch_size = 16):\n",
    "    stopper = False\n",
    "    \n",
    "    N = len(X_train.iloc[0])\n",
    "    parameters = np.random.rand(N)\n",
    "    loss_function = minus_log_cross_entropy_i if model == 'logistic' else mse_i\n",
    "    loss = 999\n",
    "    batch_idx_list = batch_idx(X_train, batch_size)\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        if stopper:\n",
    "            break\n",
    "        for idx in batch_idx_list:\n",
    "            X_batch = X_train.iloc[idx,]\n",
    "            y_batch = y_train.iloc[idx]\n",
    "            gradients = batch_gradient(X_batch, y_batch, parameters, model)\n",
    "            parameters = step(parameters, gradients, learning_rate, len(X_batch))\n",
    "            new_loss = batch_loss(X_batch, y_batch, parameters, loss_function, len(X_batch))\n",
    "            \n",
    "            \n",
    "            #중단 조건\n",
    "            if abs(new_loss - loss) < tolerance:\n",
    "                stopper = True\n",
    "                break\n",
    "            loss = new_loss\n",
    "        \n",
    "        #100epoch마다 학습 상태 출력\n",
    "        if epoch%100 == 0: #출력이 길게 나오면 check point를 수정해도 됩니다.\n",
    "            print(f\"epoch: {epoch}  loss: {new_loss}  params: {parameters}  gradients: {gradients}\")\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 위의 gradient_descent의 경우 sgd방식으로 학습할 때 parameter가 어느정도 수렴한 후 loss가 진동하는 상황이 발생하여 bgd와 mgd에서의 tolerance와 비슷한 크기로 tolerance를 설정할 경우 학습이 중단되지 않는 문제점이 있습니다. 따라서 한 epoch 전체의 loss를 계산하여 그를 통해 중단조건을 걸어주는 방식으로 코드를 바꿔보았습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_improved(X_train, y_train, learning_rate = 0.1, num_epoch = 1000, tolerance = 0.00001, model = 'logistic', batch_size = 16, verbose = 0):\n",
    "    stopper = False\n",
    "    \n",
    "    N = len(X_train.iloc[0])\n",
    "    #hyperparameter별 성능비교를 위해 seed고정\n",
    "    np.random.seed(317)\n",
    "    parameters = np.random.rand(N)\n",
    "    loss_function = minus_log_cross_entropy_i if model == 'logistic' else mse_i\n",
    "    epoch_loss = 999\n",
    "    batch_idx_list = batch_idx(X_train, batch_size)\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        new_epoch_loss = 0\n",
    "        for idx in batch_idx_list:\n",
    "            X_batch = X_train.iloc[idx,]\n",
    "            y_batch = y_train.iloc[idx]\n",
    "            gradients = batch_gradient(X_batch, y_batch, parameters, model)\n",
    "            parameters = step(parameters, gradients, learning_rate, len(X_batch))\n",
    "    \n",
    "        #epoch loss\n",
    "        new_epoch_loss = batch_loss(X_train, y_train, parameters, loss_function, len(X_train))\n",
    "            \n",
    "        #중단 조건(epoch loss)\n",
    "        if abs(new_epoch_loss - epoch_loss) < tolerance:\n",
    "            break\n",
    "        epoch_loss = new_epoch_loss\n",
    "        \n",
    "        #100epoch마다 학습 상태 출력\n",
    "        if verbose == 1:\n",
    "            if epoch%100 == 0: #출력이 길게 나오면 check point를 수정해도 됩니다.\n",
    "                print(f\"epoch: {epoch}  loss: {new_epoch_loss}  params: {parameters}  gradients: {gradients}\")\n",
    "    return parameters, new_epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement\n",
    "경사하강법 함수를 이용해 최적의 모수 찾아보세요. 학습을 진행할 때, Hyper Parameter를 바꿔가면서 학습시켜보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 1.0077190022198743  params: [0.2779874  0.71223631 0.92794493]  gradients: [0.026362909353523378, 0.014065860394253499, 0.03557708706925854]\n",
      "epoch: 100  loss: 0.4626951004337041  params: [-0.83547319  0.80861434 -0.70420743]  gradients: [0.0033643503178876016, -0.006180781536037225, 0.00726330606361644]\n",
      "epoch: 200  loss: 0.39653844744655686  params: [-1.0296477   1.35242059 -1.26854483]  gradients: [0.0012456522961709847, -0.004636173188229878, 0.004570421333485486]\n",
      "epoch: 300  loss: 0.36406830573133  params: [-1.13246683  1.75196747 -1.65912054]  gradients: [0.0008853171221793472, -0.0034608972340544235, 0.0033632407258878936]\n",
      "epoch: 400  loss: 0.34522362698202086  params: [-1.21240276  2.05790318 -1.95536704]  gradients: [0.0007272706116459082, -0.0027174078547427976, 0.0026223588522449708]\n",
      "epoch: 500  loss: 0.33320421828199137  params: [-1.27925817  2.3026999  -2.19094678]  gradients: [0.000615898366142618, -0.002213277823345135, 0.002124140839121367]\n",
      "epoch: 600  loss: 0.32503670630964476  params: [-1.33627473  2.5047809  -2.38444241]  gradients: [0.0005285965821255135, -0.001850254308872382, 0.0017676367374199933]\n",
      "epoch: 700  loss: 0.31922975974958934  params: [-1.38546696  2.67539183 -2.54711704]  gradients: [0.000458411631842069, -0.001576703926437933, 0.0015004814646821796]\n",
      "epoch: 800  loss: 0.3149592333256033  params: [-1.42832423  2.82187241 -2.68628273]  gradients: [0.0004011884120660488, -0.001363340013174037, 0.0012931307908074453]\n",
      "epoch: 900  loss: 0.31173553147931965  params: [-1.46598366  2.94927505 -2.80694876]  gradients: [0.0003539280121703924, -0.0011923988064656856, 0.001127741688507299]\n",
      "epoch: 1000  loss: 0.30925108961891645  params: [-1.49932395  3.06122742 -2.912696  ]  gradients: [0.0003144113404013376, -0.0010524967015276684, 0.0009929238645311929]\n",
      "epoch: 1100  loss: 0.3073039269182209  params: [-1.52903217  3.16042402 -3.00617319]  gradients: [0.0002809893732364334, -0.0009360034799478471, 0.0008810700265825722]\n",
      "epoch: 1200  loss: 0.30575653259217783  params: [-1.55565254  3.24892245 -3.08939508]  gradients: [0.00025242809635844283, -0.0008376116229904162, 0.0007869066771715121]\n",
      "epoch: 1300  loss: 0.30451245606418237  params: [-1.57962187  3.32833048 -3.16393023]  gradients: [0.0002277942194950556, -0.0007535131428409258, 0.0007066626698590765]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.59180641,  3.36860488, -3.20168357])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_bgd, new_epoch_loss = gradient_descent_improved(X_train, y_train,\n",
    "                                                          learning_rate = 0.1, num_epoch = 10000, tolerance = 0.00001,\n",
    "                                                          model = 'logistic', batch_size = len(X_train), verbose = 1)\n",
    "new_param_bgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.8250676854818366  params: [0.0357701  0.57414146 0.60226932]  gradients: [0.05754015449915521, 0.0539984474991064, 0.06938131607056673]\n",
      "epoch: 100  loss: 0.3087741939945116  params: [-1.55810858  3.06016902 -3.00822915]  gradients: [0.0074180006912135665, 0.010920907231229605, 0.014547327246469071]\n",
      "epoch: 200  loss: 0.30020578276493287  params: [-1.77246717  3.74270526 -3.65265103]  gradients: [0.0069735049739884005, 0.01056371950441252, 0.01307584433668057]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.83314726,  3.93282171, -3.83074835])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_mgd, new_epoch_loss = gradient_descent_improved(X_train, y_train,\n",
    "                                                          learning_rate = 0.1, num_epoch = 10000, tolerance = 0.00001,\n",
    "                                                          model = 'logistic', batch_size = 16, verbose = 1)\n",
    "new_param_mgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.4291630656003589  params: [-0.86554985  0.99220349 -1.11970185]  gradients: [0.0255864859722985, 0.013924190138782159, 0.01800241464503814]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.90439921,  4.09950491, -3.99885397])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_sgd, new_epoch_loss = gradient_descent_improved(X_train, y_train,\n",
    "                                                          learning_rate = 0.1, num_epoch = 10000, tolerance = 0.00001,\n",
    "                                                          model = 'logistic', batch_size = 1, verbose = 1)\n",
    "new_param_sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "같은 learning_rate, tolerance를 적용했을 때, epoch당 갱신수에 따라 bgd, mgd, sgd순으로 최적화가 늦게 이루어짐. 동일하게 epoch_loss를 기준으로 tolerance를 적용하였는데  최적화 방법에 따라 parameter가 꽤 상이하게 추정됨. test data에 대해 평가해보면 성능의 차이가 있을것이라 판단됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 가장 많이 쓰이는 최적화방법이고 sgd와 bgd의 장단점을 적절히 섞은 mini-batch gradient descent로 batch_size는 16으로 고정하여 learning_rate와 tolerance를 변화시켜가며 grid-search 수행( loss를 통해 비교)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearchV(X_train, y_train, X_test, y_test, hyper_params):\n",
    "    df = pd.DataFrame()\n",
    "    df.columns\n",
    "    lrs = []\n",
    "    tols = []\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    for lr in hyper_params['learning_rate']:\n",
    "        for tol in hyper_params['tolerance']:\n",
    "            new_param_mgd, train_loss = gradient_descent_improved(X_train, y_train,\n",
    "                                                     learning_rate = lr, num_epoch = 10000, tolerance = tol,\n",
    "                                                     model = 'logistic', batch_size = 16, verbose = 0)\n",
    "            \n",
    "            #predict(X_test)\n",
    "            y_pred = []\n",
    "            for i in range(len(X_test)):\n",
    "                p = logistic(X_test.iloc[i, :], new_param_mgd)\n",
    "                y_pred.append(p)\n",
    "            y_pred = pd.Series(y_pred)\n",
    "            test_loss = batch_loss(X_test, y_test, new_param_mgd, minus_log_cross_entropy_i, len(X_test))\n",
    "            \n",
    "            lrs.append(lr)\n",
    "            tols.append(tol)\n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "\n",
    "    df['learning_rate'] = lrs\n",
    "    df['tolerance'] = tols\n",
    "    df['train_loss'] = train_losses\n",
    "    df['test_loss'] = test_losses\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = {'learning_rate' : [1 * 0.5**i for i in range(5)],\n",
    "                'tolerance' : [0.0001 * 0.5**i for i in range(5)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = GridSearchV(X_train, y_train, X_test, y_test, hyper_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>tolerance</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>test_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.301599</td>\n",
       "      <td>0.252665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.301622</td>\n",
       "      <td>0.253135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.301668</td>\n",
       "      <td>0.253724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.301790</td>\n",
       "      <td>0.254756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.299742</td>\n",
       "      <td>0.255016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.299793</td>\n",
       "      <td>0.255657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.302039</td>\n",
       "      <td>0.256195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.299890</td>\n",
       "      <td>0.256474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.299175</td>\n",
       "      <td>0.256820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.299285</td>\n",
       "      <td>0.257712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.300150</td>\n",
       "      <td>0.257947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.299058</td>\n",
       "      <td>0.258404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.299541</td>\n",
       "      <td>0.259108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.299314</td>\n",
       "      <td>0.259764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.300691</td>\n",
       "      <td>0.260092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.299218</td>\n",
       "      <td>0.260109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.300054</td>\n",
       "      <td>0.261100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.299840</td>\n",
       "      <td>0.261766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.299751</td>\n",
       "      <td>0.262116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.301077</td>\n",
       "      <td>0.264065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.300890</td>\n",
       "      <td>0.264752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.300816</td>\n",
       "      <td>0.265114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.302841</td>\n",
       "      <td>0.269084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.302797</td>\n",
       "      <td>0.269471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.306389</td>\n",
       "      <td>0.275882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    learning_rate  tolerance  train_loss  test_loss\n",
       "4          1.0000   0.000006    0.301599   0.252665\n",
       "3          1.0000   0.000013    0.301622   0.253135\n",
       "2          1.0000   0.000025    0.301668   0.253724\n",
       "1          1.0000   0.000050    0.301790   0.254756\n",
       "9          0.5000   0.000006    0.299742   0.255016\n",
       "8          0.5000   0.000013    0.299793   0.255657\n",
       "0          1.0000   0.000100    0.302039   0.256195\n",
       "7          0.5000   0.000025    0.299890   0.256474\n",
       "14         0.2500   0.000006    0.299175   0.256820\n",
       "13         0.2500   0.000013    0.299285   0.257712\n",
       "6          0.5000   0.000050    0.300150   0.257947\n",
       "19         0.1250   0.000006    0.299058   0.258404\n",
       "12         0.2500   0.000025    0.299541   0.259108\n",
       "18         0.1250   0.000013    0.299314   0.259764\n",
       "5          0.5000   0.000100    0.300691   0.260092\n",
       "24         0.0625   0.000006    0.299218   0.260109\n",
       "11         0.2500   0.000050    0.300054   0.261100\n",
       "17         0.1250   0.000025    0.299840   0.261766\n",
       "23         0.0625   0.000013    0.299751   0.262116\n",
       "10         0.2500   0.000100    0.301077   0.264065\n",
       "16         0.1250   0.000050    0.300890   0.264752\n",
       "22         0.0625   0.000025    0.300816   0.265114\n",
       "15         0.1250   0.000100    0.302841   0.269084\n",
       "21         0.0625   0.000050    0.302797   0.269471\n",
       "20         0.0625   0.000100    0.306389   0.275882"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.sort_values(\"test_loss\").head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "learning_rate\n",
       "1.0000    0.254095\n",
       "0.5000    0.257037\n",
       "0.2500    0.259761\n",
       "0.1250    0.262754\n",
       "0.0625    0.266538\n",
       "Name: test_loss, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.groupby(\"learning_rate\")['test_loss'].mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tolerance\n",
       "0.000006    0.256603\n",
       "0.000013    0.257677\n",
       "0.000025    0.259237\n",
       "0.000050    0.261605\n",
       "0.000100    0.265063\n",
       "Name: test_loss, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.groupby(\"tolerance\")['test_loss'].mean().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과적으로 주어진 하이퍼파라미터들 중 learnsing_rate가 가장 클 때(1)와 tolerance가 가장 작을때의 성능이 가장 좋았고, 반대로 learning_rate가 가장 작을때(0.0625)와 tolenrance가 가장 클때의 성능이 가장 좋지 않았음\n",
    "\n",
    "전반적으로도 learning_rate가 클때의 test_loss가 낮고, train_loss는 높은 경향을 보였는데 이는 너무 작은 learning rate는 오히려 트레인셋에 과적합이 되어 나타나는 현상으로 판단된다. 또한, train_loss와 test_loss모두에서 tolerance가 작을때 더 좋은 성능을 보였는데 tolerance가 너무 클 경우 데이터를 충분히 학습하지 못한 결과라고 판단된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이후 test_loss가 가장 낮았던 hyper parameter(lr = 1, tol = 6.25e-06)로 최적화를 진행한 후 threshold 0.5를 기준으로 예측을 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.4780757669162645  params: [-0.91391092  0.56678884 -0.94219864]  gradients: [0.17060485097826406, 0.17683594908912809, 0.2819733338847531]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-1.99410758,  4.11815288, -4.15031133]), 0.3015989579235988)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_mgd, train_loss = gradient_descent_improved(X_train, y_train,\n",
    "                                                     learning_rate = 1., num_epoch = 10000, tolerance = 6.25e-06,\n",
    "                                                     model = 'logistic', batch_size = 16, verbose = 1)\n",
    "new_param_mgd, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = []\n",
    "for i in range(len(y_test)):\n",
    "    p = logistic(X_test.iloc[i,:], new_param_mgd)\n",
    "    if p> 0.5 :\n",
    "        y_predict.append(1)\n",
    "    else :\n",
    "        y_predict.append(0)\n",
    "y_predict_random = []\n",
    "for i in range(len(y_test)):\n",
    "    p = logistic(X_test.iloc[i,:], random_parameters)\n",
    "    if p> 0.5 :\n",
    "        y_predict_random.append(1)\n",
    "    else :\n",
    "        y_predict_random.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[37,  3],\n",
       "       [ 2,  8]], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_predict).ravel()\n",
    "confusion_matrix(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9\n",
      "sensitivity : 0.8\n",
      "specificity : 0.925\n"
     ]
    }
   ],
   "source": [
    "accuracy = (tp+tn) / (tp+fn+fp+tn)\n",
    "print(\"accuracy:\", accuracy)\n",
    "print(\"sensitivity :\", tp / (tp + fn))\n",
    "print(\"specificity :\", tn / (fp + tn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "도출된 parameter를 통해 예측하고 정확도, 민감도, 특이도를 통해 성능을 측정한다.\n",
    "정확도와 특이도는 양호한 편이나 실제 1중 1로 예측한 비율인 민감도가 현저히 떨어지는것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "### $y = 0.5 + 2.7x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_X = np.random.rand(150)\n",
    "y = 2.7*raw_X + 0.5 + np.random.randn(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.array([1 for _ in range(150)])\n",
    "X = np.vstack((tmp, raw_X)).T\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.Series(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정규방정식을 이용하면 $\\hat{\\beta} = (X^TX)^{-1}X^Ty$식으로 회귀계수를 추정할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17435363, 3.20029089])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#정규방정식\n",
    "theta = np.linalg.inv(np.dot(X.T,X)).dot(X.T).dot(y)\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.7121655155892971  params: [0.73391864 1.03499601]  gradients: [-0.07087725218079671, -0.051732252366537154]\n",
      "epoch: 100  loss: 0.3882913835731118  params: [0.22658268 3.12437596]  gradients: [-0.00903829194531716, -0.005978745189347095]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.19136 , 3.190181])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#경사하강법\n",
    "new_param, loss = gradient_descent_improved(X, y, learning_rate = 0.1, num_epoch = 10000, tolerance = 0.000001,\n",
    "                                      model = 'linear', batch_size = 32, verbose = 1)\n",
    "new_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정규방정식을 통해 추정한 회귀계수와 경사하강법을 통해 추정한 회귀계수를 통해 예측을 수행한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_NE = theta.dot(X.T)\n",
    "y_hat_GD = new_param.dot(X.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "시각화를 통해 정규방정식과 경사하강법을 통한 선형회귀를 비교해보세요  \n",
    "(밑의 코드를 실행만 시키면 됩니다. 추가 코드 x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(X.iloc[:,1], y, '.k') #산점도\n",
    "plt.plot(X.iloc[:,1], y_hat_NE, '-b', label = 'NE') #정규방정식\n",
    "plt.plot(X.iloc[:,1], y_hat_GD, '-r', label = 'GD') #경사하강법\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파란선은 정규방정식을 통해 추정한 회귀선이고 빨간선은 경사하강법을 통해 추정한 회귀선이다. 회귀계수 측면에서 미세한 값의 차이가 있지만 시각화해보았을때 큰 차이가 없는것을 알 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
